fork()

Шелл-трюки, выпуск 5
Со скоростью мысли, выпуск 1

#шелл_трюки
#cо_скоростью_мысли


Совсем недавно, а с учётом масштаба произошедшего события можно сказать, что вот только что,
в марте 2015 года, Гордон Мур, сооснователь Intel Corporation заявил, что закон Мура,
который гласит, что количество транзисторов на кристалле интегральной микросхемы,
удваивается каждые два года, перестаёт действовать и скоро достигнет точки насыщения,
где перестанет действовать окончательно. И это ещё бы ничего, но заканчивается и постоянный
рост производительности процессоров, который напрямую связан с числом транзисторов на кристалле. 

Одним словом, шара заканчивается.

Что это означает? Последствий множество, но наиболее очвидное, что закон природы, гласящий
"если программа медленная, нужно подождать немного и на следующем компьютере программа станет быстрой"
оказался вовсем не законом природы, а кратковременным, хотя и приятным феноменом.

Отныне быстрые программы будут оставаться быстрыми, а медленные — медленными.

Это в свою очередь означает, что умение выжать из железа максимум снова становится востребованным.
Особенно остро это ощущается в сфере встраеваемых (embedded) устройств,
где количество ресурсов, в первую очередь энергии, сильно ограничено,
и в облачных системах, где за все потребляемые ресурсы нужно отдельно платить,
и любая неэффективность софта фактически автоматически обозначает лишние затраты.

К своему успокоению надо сказать, что десятилетия беспрекословного соблюдения закона Мура
настолько разбаловали всех, и ресурсы используются на всех уровнях настолько расточительно,
что оптимизация в разы и даже на порядки оказывается, порой, совсем не трудным делом.
И даже простейших манипуляций с шеллом, о чём сегодня и пойдёт речь,
оказывается достаточно, чтобы повысить производительность системы значительно.

Прежде чем перейти к сути сегодняшнего вопроса, а говорить мы сегодня будем о системном вызове fork(),
отмечу только, что основные направления повышения производительности систем,
по которым и будет собственно вестись работы в ближайшие годы, можно разделить на три группы:

1. Усовершенствование алгоритмов, переход с алгоритмов O(n*n) на O(n*log n) и O(n) на O(log n);
2. Написание качественного кода, рациональное использование ресурсов, тонкая настройка системы;
3. Распараллеливание задач и использование распределённых, параллельных и конкурентных алгоритмов, а так же языков и библиотек, которые облегчают это использование.

С первым пунктом всё очевидно, и именно он по сути является решающим в создании высокопроизводительных систем.
Однако против пузырьковой сортировки и закон Мура бессилен, поэтому и в прошлые годы он был весьма актуален.

Третий пункт сейчас получил особую важность, поскольку первым остановился рост тактовой частоты процессора,
которая позволяла ускорять программы вообще ничего не меняя в них. Они просто автоматически становились быстрее.
Сейчас же вычислительная мощность хотя и по прежнему растёт, но доступна она не в виде одного ещё более быстрого ядра,
а в виде большего количества таких же или даже более медленных ядер, что не предполагает роста производительности
при сохранении архитектуры программы. Нужны совсем другие подходы, техники, структуры данных, и часто даже языки,
для того чтобы воспользоваться всей мощностью нового процессора.

Второй пункт хотя и не позволяет получить такого большого прироста производительности как первый,
но когда уже и так используются оптимальные алгоритмы (а зачастую это именно так), внимание фокусируется
на конкретной программной реализации, и на его окружении. И именно здесь становятся особенно важными
профессионализм, знания и опыт инженера или программиста. Именно на этом пункте мы и сконцентрируемся
в ближайших выпусках новой серии.

Сегодняшний, первы выпуск серии, мы посвящаяем краеугольному камню
UNIX/Linux-систем, системному вызову fork(), и сделаем это в контексте поставленной нами задачи,
задачи повышения производительности наших систем. Но и про shell-трюки не забудем.

Как и полагается при форке, серия шелл-трюков тут разделяется и превращается в две:
старые добрые "шелл-трюки", и новую серию "со скоростью мысли".

* fork

fork это, наверное, одна из самых интересных вещей, которая есть в UNIX,
и уж точно самый интересный системный вызов. В самом деле, read, write,
open, close — всё банально. Не то fork — в нём сокрыта великая тайна, тайна рождения процессов.
Это чувствовал и помнит каждый, кто когда-то в своё время разбирался с кодом,
ставя себя на место программы (а точнее, процесса). 

Вот тут я читаю, так, тут пишу, тут открываю файл, опять читаю, тут закрываю,
прибавляю один, так, идём дальше, тут я делаю форк, форкаюсь, тут... стоп, где это тут?
где я? тут или тут? или меня уже двоя? стоп, стоп, назад...

if (fork() == 0) {
  // тут я ребёнок
}
else {
  // а тут я по прежнему я
  // стоп... но то ведь тоже я?
}

И тут вы понимаете, что это какое-то особенное место в системе, очень важное,
даже таинственное, и что сейчас, может быть, не стоит пытаться понять это полностью
(как не нужно понимать тайну жизни, для того чтобы просто жить), а нужно просто
принять это как данность, и разбираться с кодом дальше, просто считая, что вы то тут,
то там, и вообще вас много, ну а что тут такого? Однако трепет, благоговение
передо форком и понимание его особой роли остаются.

Форк, по-видимому, действительно был одним из важнейших архитектурных решений в UNIX,
которому UNIX и обязан своей высочайшей популярностью. Он уже определил на многие десятилется архитектуру
и устройство UNIX, и, конечно, ещё не конец.

Кстати сказать, в отличие от многих вещей, впервые появившихся в UNIX, а потом перешедших в другие системы,
таких, например, как язык Си или иерархическая система каталогов (хотя если быть точным,
то она появилась в его предшественнике Multics, а бумажные версии были и ранее, например в Marc 1 в 1958 [http://programmers.stackexchange.com/questions/103487/]),
форк появился впервые в другой операционной системе, Genie, над которой в своё время работал Кен Томпсон,
позже перенёсший его идею в UNIX.

Форк в UNIX не только есть, но он ещё и очень легковесен и эффективен. Если в других системах
того времени, например, VMS, порождение нового процесса было целой историей, то в UNIX
это происходило легко и непринуждённо, одним системным вызовом, так же легко,
как открыть или закрыть файл или прочитать или записать в него данные. Это позволило появиться
сегодням всем нам хорошо известному стилю *unixway*, когда вместо одной большой программы-монстра,
предназначенной для решения всего-всего-всего (путь по которому пошёл VMS и позже его последователи, в том числе Windows),
существует огромнейшее количество маленьких программ, каждая из которых хорошо умеет выполнять отдельное действие,
и они объединяются между собой для решений какой-то определённой задачи.
Объединение это выполняется (как правило, но не обязательно) средствами шелла,
который соединяет их между собой несколькими стандартными способами, 
в первую очередь пайпами (pipes), так же известными как каналы, командными подстановками
и файлами (в том числе безымянными, существующими только в виде открытых дескрипторов).

Конечно, всё это возможно, только благодаря тому, что (1) форк существует; (2) форк очень дёшев.
Если бы форк был дорогим и занимал бы секунды, то простейшый скрипт, который выполняет сотни или тысячи 
форков, работал бы часами вместо того, чтобы завершаться мгновенно.

Этот факт, очень дешёвого форка, известен любому юниксоиду с первого класса, и он настолько прочно
вошёл в его сознание, что в результате возникает другая крайность — форки делаются тысячами и сотнями тысяч,
и там, где без них совсем можно обойтись.

* Два примера с форком и без

Для начала очень красноречивый (и настолько же бесполезный) пример:

$ time for i in {1..10000}; do true; done

real    0m0.034s
user    0m0.028s
sys     0m0.004s

$ time for i in {1..10000}; do /bin/true; done

real    0m5.644s
user    0m2.240s
sys     0m1.544s

Как так? Одно и то же бесполезное действие выволняется в одном случае почти в 200 раз быстрее чем в другом?
Вы уже, безусловно, догадались в чём дело, но давайте, однако, возьмём какой-нибудь пример попрактичнее
и погрузимся в детали.

Возьмём простую задачу: нужно скопировать десять тысяч маленьких файлов из одного каталога в другой.

$ touch {1..10000}
$ mkdir d

В простейшем случае мы можем скопировать все файлы просто одним скопом:

$ cp [0-9]* d/

Это займёт у нас мгновение ока, а точнее:

$ time cp [0-9]* d/

real    0m0.135s
user    0m0.036s
sys     0m0.096s

Но что если мы не можем скопировать их все одним разом, а копирование выполняется по одному?
Например, если мы хотим проверять что-то для каждого файла, или просто потому что операции
копирования разбросаны в разных частях скрипта (если вам задача кажется надуманной, вы ошибаетесь;
можно привести множество примеров, где нужно выполнить именно это; наиболее очевидный примеров
это сборочная система, которая должна инсталлировать только определённые файлы).

Итак, сколько потребуется времени чтобы проинсталлировать эти файлы поотдельности?

$ for f in *; do cp $f d/; done

Или с замером времени:

$ time for f in [0-9]* ; do cp $f d/; done

real    0m16.141s
user    0m3.152s
sys     0m2.564s

16 секунд?! Жесть какая-то. То есть, просто потому что мы копируем не все файлы сразу,
а по отдельности это получается в сто раз медленнее?!! (конечно, такая потеря производительности
будет только на пустых файлах; на файлах большего размера потеря будет, разумеется, меньше) 

Частично виноват ещё сам шелл и его реализация,
так, например, заменой интерактивной версии на неинтерактивную можно выиграть 30 процентов:

$ time bash -c 'for f in [0-9]* ; do cp $f d/; done'

real    0m10.544s
user    0m1.988s
sys     0m1.388s

И заменой bash на Bourne Shell выиграть ещё процентов 10:

$ time sh -c 'for f in [0-9]* ; do cp $f d/; done'

real    0m8.414s
user    0m0.152s
sys     0m1.064s

Но всё же всё это ерунда по сравнению с потерей производительности почти в 50 раз.

В чём же тут дело, и как можно исправить ситуацию?

Проблема как раз и заключается в том, что для выполнения каждой операции копирования
нужно выполнить форк и запустить отдельный процесс для программы /bin/cp.
Когда мы вызываем программу со всеми файлами сразу, она один раз запускается
и начинает копировать их. Обработка каждого файла происходит внутри программы,
не требуя порождения отдельного процесса. Когда же мы делаем это из шелла, то для каждого
файла программа должна запускаться заново, и каждый запуск сопровождается форком.

* Как подсчитать форки

Как мы можем это проверить?

Способов несколько.
Простейший это подсчитывать количество порождённых дочерних процессов:

$ set -o monitor
$ FORKS=0
$ trap "((++FORKS))" CHLD
$ for f in [0-9]* ; do cp $f d/; done
$ echo $FORKS
10000

При каждом завершении порождённого дочерднего процесса выполняется обработчик,
повешенный на сигна SIGCHLD (trap "((++FORKS))" CHLD). В нашем случае это увеличение переменной FORKS на один.

Этот метод работает, потому что мы запускаем процессы непосредственно из текущего шелла.
В жизни всё бывает сложнее, а именно, что процессы запускаются не самим процессом, а одним из его подроцессов,
и часто даже через несколько поколений. Как же подсчитать форки в этом случае?

Используем трассировку системных вызовов с помощью *strace*:

$ strace -f sh -c 'for f in [0-9]* ; do cp $f d/; done' 2>&1 | grep execve | wc -l
10021

Здесь мы считаем обращение к функции *execve*, за которой и стоят форки в итоге.
По дороге было несколько запусков и дозапусков из-за этого число получилось большим,
но переменная составляющая, определяющаяся количеством копируемых файлов тут, очевидно, тоже присутствует.

Обратите внимание на ключ `-f`, который здесь чрезвычайно важен. Он говорит, что трассировку
нужно выполнять не только в самом процессе, но и в его потомках.

* Как оставить ту же логику, но поднять производительность

Можно ли как-то одним махом избавиться от форков?

Не во всех случаях, но можно, и часто даже очень просто. В рассматриваемом нами примере
это сделать вообще элементарно:

$ strace -f busybox sh -c 'for f in [0-9]* ; do cp $f d/; done' 2>&1 | grep execve | wc -l
1

Вместо 10021 форка один форк! И ещё не сразу заметишь, что же поменялось в команде!!

Разница в том, что вызывается не обычный шелл (Bourne Shell, sh), а BusyBox Shell (busybox sh).
Для этого перед командой `sh` добавлено слово `busybox`.

Проект BusyBox хорошо известен многим как проект "множество программ в одной".
Огромнейшее количество программ (в первую очередь coreutils, но не только) собраны в одном бинарнике (иногда даже статически, 
но статически или динамически, зависит от конфигурации при сборке). Какую роль играет сейчас бинарник (cp, mv, sh, или что-то другое)
зависит от того, по какому имени он вызывается. В итоге получается, что самые главные утилиты системы
размещены у вас в одной программе. Это очень важно для систем с ограниченными ресурсами, прежде всего embedded,
но как мы теперь видим, не только.

Для нас тут важным является то, что поскольку весь код размещён внутри одного бинарника,
нам не нужно запускать новых программ. То есть, cp, mv и так далее для busybox получаются
просто встроенными командами, builtins (как true для bash в заглавном примере).

Это позволяет отказаться от форков и сэкономить огромное количество ресурсов.
Естественно, что всё это происходит прозрачно. Так, например, если вы до этого запустили
busybox shell и выполняете команду в нём, то она уже автоматически использует
встроенные команды:

$ busybox sh
$ for f in [0-9]* ; do cp $f d/; done
$ time sh -c 'for f in [0-9]* ; do cp $f d/; done'
real    0m 2.21s
user    0m 0.11s
sys     0m 0.61s

Здесь sh это тоже встроенная команда, поэтому запуская sh -c мы имеем в виду шелл busybox (но один форк в этом случае,
конечно, всё равно произойдёт).

Вот так вот на ровном месте, просто запустив скрипт в другом шелле, мы понизили требуемые ресурсы почти на порядок
(с 16 секунд до 2), ничего не меняя при этом в логике исполнения, и вообще не трогая скрипт ни одним пальцем.

Прежде чем идти дальше и продолжать углубляться в тему отказа от форков в других, более сложных случаях,
остановимся ещё на шелле.

Минимизация или полный отказ от внешних процессов в шелле возможен во множестве ситуаций.
Например, в современных шеллах можно часто обойтись без использования grep для проверки
условий, используя вместо этого встроенную операцию проверки регулярного выражения [[ =~ ]] .

Допустим, нам нужно выбрать только те файлы, которые содержат две или три единицы в названии:

$ for i in *; do [[ $i =~ 1{2,3} ]] && echo $i; done

Или, чтобы не портить вывод, вообще не будем выводить их имён (echo это builtin поэтому не влияет на скорость):

$ time for i in *; do [[ $i =~ 1{2,3} ]]; done

real    0m0.084s
user    0m0.076s
sys     0m0.008s

Если же делать это с grep (в данном случае egrep, потому что мы используем расширенные регулярные выражения, {}):

$ for i in *; do echo $i | egrep -q '1{2,3}' && echo $i; done

Или без вывода: 

$ time for i in *; do echo $i | egrep -q '1{2,3}'; done

real    0m19.198s
user    0m2.064s
sys     0m3.360s

Разница в 250 раз! И разница тут только в форках.

В последние годы в шелле появилось очень много удобных механизмов, типа названных выше, которые позволяют
отказаться от использования внешних программ, или по крайней мере существенно минимизировать их использование.
Во многих случаях готовые встроенные команды уже есть, но если их нет,
их можно загрузить или даже написать свои собственные (в том случае, если выигрыш будет очень существенным,
и оно будет стоить того; а надо сказать, что такое вполне может случиться, например, вместо того чтобы 
переписывать существующие шелловские скрипты на другом языке программирования, достаточно просто найти
какие команды вызываются особенно часто и заменить их загружаемыми модулями).

Итак, мы увидели, насколько легко и насколько сильно можно увеличить производительность
в шелловских скриптах, где это особенно важно, потому что являясь связующим языком (glue language),
шелл особенно склонен к тому, чтобы запускать множество вспомогательных программ на каждом шагу.
Поэтому, оптимизационный потенциал шелла здесь особенно велик. Однако, это не единственное место,
где дочерние процессы запускаются тысячами и даже миллионами.

Но прежде чем идти дальше ответим на другой, более фундаментальный вопрос:

* Зачем нужны все эти крысиные бега?

Хорошо, мы научились выжимать 20 секунд из копирования десяти тысяч пустых файлов.
Экое достижение! Но нужно ли это на самом деле, и какой практический смысл могут иметь такие гонки
кроме собственно спортивного интереса? Не идём ли мы по пути вошедшего в историю средневекового конторщика,
который работая с золотыми монетами, постоянно покупал новую скатерть, сжигал старую и из её пепла
насеивал потом себе целое состояние золотых крупиц обтиравшихся у него на столе монет?
И если да, на какие груды золота мы в итоге можем рассчитывать?

Давайте рассмотрим простой пример: какая-нибудь средняя компания, насчитывающая 100 активных разработчиков,
работает над своим собственным продуктом. Каждый разработчик делает изменения в своём собственном бранче,
время от времени собирая и тестируя его. Одним словом, самая обычная ситуация. Количество сборок за день
может сильно варьироваться, определяясь скоростью сборки, размером проекта, манерой работы, принятыми процессами
и множеством других факторов, но пусть это будет реалестичное число в 20 сборок в день на разработчика.
Если предположить, что в результате оптимизаций сборочной системы получится сэкономить в среднем до минуты
на сборке, то в сумме это будет 2000 минут в масштабах компании, или для ровного счёта 30 часов в день.

Понятно, что в жизни всё намного сложнее, и само по себе сэкономленное на сборке время совершенно
не обязательно приведёт к соответствующему повышению производительности разработки,
но в любом случае намного приятнее работать с системой, которая летает чем тупит.

* Sharedlib busybox

Каким же образом решить задачу минимизации форков в других программах, не в шелле?
Например, если мы говорим о сборочной системе, то это будет всё, что угодно, make, waf, ninja,
но никак не шелл. Как можно сделать так, чтобы некая программа, скажем ninja, 
не выполняла форки на каждом шагу, а старалась делать всё сама?

Да.

BusyBox поддерживает режим не только отдельного бинарника (standalone binary), но и режим разделяемой
библиотеки `libbusybox.so` (если собран соответствующим образом). Его функции можно вызывать 
прямо непосредственно из кода, не выполняя никаких форков вообще!

Вот, например, как скопировать с его помощью файл:

#include <stdio.h>

extern int lbb_main(char **argv);

int main()
{
    int i;
    char* strarray[] = {"cp", "1", "d/1", 0};
    lbb_main(strarray);

    return 1;
}

$ gcc -o cpfile -lbusybox -L. cpfile.c
$ rm d/1
$ ./cpfile
$ ls d/1
d/1

(тут есть несколько моментов, которые нужно иметь в виду, в частности, что lbb_main в конце по умолчанию вызывает exit,
поэтому программа не дойдёт до return 1, но принципиально это ничего не меняет).

Таким образом, вместо вызова внешних программ через `execve` вы просто обращаетесь к функциям busybox,
прилинкованного прямо в программу.

Откуда он возьмётся в уже готовой и существующей программе, make или ninja?

Есть два варианта: первый, быстрый, второй, более качественный.

В первом случае libbusybox, точнее, библиотека построенная на его основе, загружается с помощью LD_PRELOAD.
Вызовы execve перехватываются, и если это известные вызовы, такие как `cp` или `chmod`, они обрабатываются функциями busybox,
если же нет, то выполняется обычный классический execve с форком. Этот способ очень быстрый, и не требует
никаких модификаций собственно кода программы. 

Второй способ это интеграция libbusybox непосредственно в код программы. Это решение более зрелое и основательное.
В идеале патч может быть даже интегрирован в мэйнстрим, и он станет доступен всем пользователям программы.

* А можно ли ещё быстрее? 

Отказ от использования форков хотя и очень существенно может повысить производительность,
но не решает всех проблем. Куча бесполезной работы выполняется в любом случае, даже если мы её не видим.

Насколько быстро можно было бы скопировать файлы в уже знакомом нам примере?
Мы помним, что с помощью cp мы смогли это сделать за 0.135s, с помощью busybox за 2.2s:

$ time cp [0-9]* d/

real    0m0.135s
user    0m0.036s
sys     0m0.096s

$ time sh -c 'for f in [0-9]* ; do cp $f d/; done'
real    0m 2.21s
user    0m 0.11s
sys     0m 0.61s

Каков теоретический максимум? Сколько можно было бы выжать, если нажать на полный газ?


#define _GNU_SOURCE
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <stdio.h>
#include <unistd.h>
#define splice(a, b, c) splice(a, 0, b, 0, c, 0)

int main()
{
    char infile[4096];
    char outfile[4096];
    for(int i=1; i<=10000; i++) {
        sprintf(infile, "%d", i);
        sprintf(outfile, "d/%d", i);
        int p[2];
        pipe(p);
        int out = open(outfile, O_WRONLY | O_CREAT, 0640);
        int in = open(infile, O_RDONLY);
        while(splice(p[0], out, splice(in, p[1], 4096))>0);
        close(in);
        close(out);
    }
    return 0;
}

На пустых файлах разница получается почти в два раза:

$ time ./mycp 

real    0m0.065s
user    0m0.008s
sys     0m0.056s

На файлах большего размера разница становится менее существенной.

Чуда не произошло. Как и следовало ожидать, программа `cp` оказалась вполне эффективной.
Чем же объясняется выигрыш в два раза, который показала наша программа?
Ответ очевиден: она не делала никаких дополнительных действий типа проверок на существование файлов,
ошибок при создании и тому подобного, и второе — она знала какие именно файлы и куда ей нужно копировать,
ей не нужно было никак парсить командную строку и вообще ничего делать кроме просто копирования.

Хорошо было бы, если бы так было в реальных программах! Всё известно заранее.
И оказывается такое и правда бывает!!

В некоторых случаях оказывается значительно быстрее и эффективнее на основе исходных данных
сначала сгенерировать и откомпилировать программу, которая потом просто уже будет выполняться
по захардкоженным в неё данным, не задумчиво интерпретируя входные данные, а слепо отрабатывая
заложенную в неё логику.

В простейших случаях это может быть кодогенерация с помощью препроцессоров Си,
но особенно ярко и эффективно это проявляется в языках программирования, которые поддерживают
развитую систему макросов, в первую очередь в Lisp.

Мы отдельно остановимся на этом в наших следующих выпусках серии #со_скоростью_мысли,
посвящённых вопросам производительности и скорости исполнения.

